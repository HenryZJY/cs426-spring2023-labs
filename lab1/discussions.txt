# A1
Dial() is similar to connect() in network socket API. Like connect(), Dial() is used on the client side to establish a connection 
with the server on the given port number. 

# A2
gRPC's Dial function can fail for a variety of reasons including:

1. Network issues: The target host cannot be reached or there is a problem with the network connection. In these cases, 
we can return a UNAVAILABLE status code.

2. Incorrect target address: The target address is incorrect or the target host is not running the gRPC server. In this case, 
we can return a NOT_FOUND status code.

3. TLS authentication failure: If the gRPC client is using TLS and the server requires client authentication, 
and the client's certificate is not recognized by the server, Dial will fail. we can return a UNAUTHENTICATED status 
code in this case.

4. Timeout: If the dial operation takes too long to complete, it may time out. In this case, we can return a 
DEADLINE_EXCEEDED status code.

# A3
When calling GetUser() service, the gRPC library will make use of several network and system calls. 
Some of the calls include:

1. connect(): This system call is used to establish a connection to the target gRPC server. This call can fail if 
the target host is unreachable or if there is a problem with the network connection.

2. send() and recv(): These system calls are used to send and receive data over the network connection. 
They can fail if the connection is closed or if there is a problem with the network.

3. poll() or select(): These system calls are used to monitor the network connection for incoming data. 
They can fail if there is a problem with the file descriptor being monitored.

The GetUser() call itself can also return errors in several cases: 

1. Unavailable server;
2. Malformed request;
3. Unauthenticated request;
4. Deadlines exceeded;
5. Resource exhaustion.


# ExtraCredit1:
These errors can be detected in several ways:
1. Status codes
2. Exception handling
3. Error return values in functions
4. Callback functions

# A4
If we use the same conn to fetch videos, we will get an error when calling the video services. The conn we used
for user service is created by dialing the user server address, not the video server address. 


# A6
Welcome jz792! The UserId we picked for you is 200033.

2023/02/04 17:53:03 This user has name Franecki6640, their email is izaiahkeeling@schmitt.info, and their profile URL is https://user-service.localhost/profile/200033
2023/02/04 17:53:03 Recommended videos:
2023/02/04 17:53:03   [0] Video id=1147, title="Importanceshake: copy", author=Luciano Padberg, url=https://video-data.localhost/blob/1147
2023/02/04 17:53:03   [1] Video id=1055, title="envious Leeks", author=Wyman Wisoky, url=https://video-data.localhost/blob/1055
2023/02/04 17:53:03   [2] Video id=1057, title="The lively ant's advice", author=Izaiah Keeling, url=https://video-data.localhost/blob/1057
2023/02/04 17:53:03   [3] Video id=1003, title="thankful east", author=Andre Kassulke, url=https://video-data.localhost/blob/1003
2023/02/04 17:53:03   [4] Video id=1024, title="Goldfishbow: generate", author=Lavonne Bergstrom, url=https://video-data.localhost/blob/1024

# A8
I decided not to call the batched requests concurrently, since concurrency in this case can have some disadvantages:

1. Increased complexity: Managing concurrent requests can increase the complexity of the system and make it more 
difficult to reason about its behavior.

2. Risk of overloading the server: Sending too many concurrent requests can result in overloading the target server, 
leading to increased latencies or even failures.

3. Increased memory usage: Sending requests concurrently may require additional memory to store the request state 
and the results of the requests.


# ExtraCredit2

# B2
go run cmd/loadgen/loadgen.go --target-qps=10
total_sent:20   total_responses:19      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:112.84
total_sent:30   total_responses:29      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:101.48
total_sent:39   total_responses:38      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:106.55
total_sent:49   total_responses:48      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:115.81
total_sent:59   total_responses:58      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:118.67
total_sent:69   total_responses:68      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:121.54
total_sent:80   total_responses:79      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:121.38
total_sent:89   total_responses:88      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:122.95
total_sent:99   total_responses:98      total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:124.32
total_sent:109  total_responses:108     total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:125.66
total_sent:119  total_responses:118     total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:125.92
total_sent:129  total_responses:128     total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:126.76
total_sent:140  total_responses:138     total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:127.22
total_sent:149  total_responses:148     total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:127.84
total_sent:159  total_responses:158     total_errors:0  failure_rate:0.00%      stale_responses:0       avg_latency_ms:128.35

# C1
Retrying may be bad for the following reasons:

1. If the root cause of a request failure is not resolved, 
retrying the request may result in an infinite loop of retries, leading to increased latencies or even resource exhaustion.
2. Retrying a request multiple times can result in overloading the target server, 
leading to increased latencies or even failures.
3. Retrying a request multiple times can waste valuable system resources such as network bandwidth and CPU cycles.
4. Retrying a request may result in an inconsistent state, especially if the request modifies the state of the system.

In some cases we should not retry requests: 
1. If a request has side-effects, such as modifying the state of the system, retrying the request may result in undefined behavior. 
2. If a request fails due to a non-recoverable error, retrying is simply a waste of resource. 
3. If a request has a tight deadline, retrying the request may result in missing the deadline.


# C2
I believe we should return the expired videos instead of returning an error. A reliable server should always
try to offer the users something as long as there are some reasonable resources available. From a user's point 
of view, seeing expired videos is still better than seeing an error. 


# C3
Instead of recommending trending videos, we can also cache a small part of the previous recomended video ids for 
every user. This way, the fallback recommendation are more personalized than trending videos, but there would be more 
space overhead from caching video ids for every user. 


# C4
The cost of connection can be high in a high-throughput service especially if the connection 
establishment is repeated for each request. This is because there are several steps in the process
 of establishing a connection, including DNS resolution, TCP handshake, SSL/TLS negotiation. These
steps can add a lot of latencies to the requests. 

To avoid per-request connection establishment, we can reuse existing connections for multiple requests. 
We can do this by using connection pooling. There are also tradeoffs to consider when 
using connection pooling, including: 
1. Increased space overhead for maintaining the pool. 
2. Connection overhead: If the server is frequently restarted or if the network topology changes, 
re-establishing the connection pool can be costly. 
3. Increased risk of connection failure due to reusing connections. 